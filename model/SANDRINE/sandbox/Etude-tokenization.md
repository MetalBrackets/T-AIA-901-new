# Recherche pour tokenizer

-> install

```
pip install tensorflow pandas numpy transformers
pip install pandas transformers sacremoses nltk
```

## Phrases

üìÑ utilisation du fichier reservation-first-dataset-train.csv

```csv
# ex :
Phrase,D√©part,Arriv√©e
"j'aimerais voyager de Ailly-sur-Somme √† Aix-les-Bains le Revard","Ailly-sur-Somme","Aix-les-Bains le Revard"
"quels trains voyagent d'Alen√ßon √† Corbeil-Essonnes","Alen√ßon","Corbeil-Essonnes"
```

### Script 1

Tokenisation avec `word_tokenize` de `NLTK`  
-> utilise simplement str.split()  
**Probl√®mes**

- Les contractions comme "d'Alen√ßon" ne sont pas g√©r√©es
- Ne trouve pas l'entit√©

```json
   {
    "tokens": ["j'aimerais", "voyager", "de", "Ailly-sur-Somme", "√†", "Aix-les-Bains", "le", "Revard"],
    "labels": ["O", "O", "O", "B-DEP", "O", "B-ARR", "I-ARR", "I-ARR"]
  },
  {
    "tokens": ["quels", "trains", "voyagent", "d'Alen√ßon", "√†", "Corbeil-Essonnes"],
    "labels": ["O", "O", "O", "O", "O", "B-ARR"]
  }
```

### Script 2

Introduction d'un `RegexpTokenizer` personnalis√© pour g√©rer les contractions fran√ßaises  
REGEX : `r"\b(?:d'|l'|j'|qu'|n'|s'|t'|m'|c')?\w+\b"`  
Ce tokenizer tokenise √† la fois les phrases et les mots  
ex: Ailly-sur-Somme est d√©coup√© en 3 tokens

**Probl√®mes**

- Contractions non s√©par√©es (ex: "d'Alen√ßon" est un seul token)
- Erreur d'annotation

```json
  {
    "tokens": ["j'aimerais", "voyager", "de", "Ailly", "sur", "Somme", "√†", "Aix", "les", "Bains", "le", "Revard"],
    "labels": ["O", "O", "O", "B-DEP", "I-DEP", "I-DEP", "O", "B-ARR", "I-ARR", "I-ARR", "I-ARR", "I-ARR"]
  },
  {
    "tokens": ["quels", "trains", "voyagent", "d'Alen√ßon", "√†", "Corbeil", "Essonnes"],
    "labels": ["O", "O", "O", "O", "O", "B-ARR", "I-ARR"]
  }
```

### Script 3

Am√©lioration
REGEX : `r"\b(?:d'|l'|j'|qu'|n'|s'|t'|m'|c')\b|\b\w+\b"`

- S√©pare les contractions en deux tokens distincts ("d'" et "Alen√ßon")

Tr√®s bon r√©sultat

```json
  {
    "tokens": ["j'", "aimerais", "voyager", "de", "Ailly", "sur", "Somme", "√†", "Aix", "les", "Bains", "le", "Revard"],
    "labels": ["O", "O", "O", "O", "B-DEP", "I-DEP", "I-DEP", "O", "B-ARR", "I-ARR", "I-ARR", "I-ARR", "I-ARR"]
  },
  {
    "tokens": ["quels", "trains", "voyagent", "d'", "Alen√ßon", "√†", "Corbeil", "Essonnes"],
    "labels": ["O", "O", "O", "O", "B-DEP", "O", "B-ARR", "I-ARR"]
  }
```

### Script 4

Observer la sortie des tokenizeurs entrain√©s sur du fr avec des lib d'Hugging Face. Le pipeline NER ne peut pas deviner par lui m√™me le d√©part et l'arriv√© sans √™tre entrain√© sur les donn√©es.

Chaque tokenizeur a sa propre strat√©gie de segmentation :

- CamemBERT et FlauBERT -> optimis√©s pour le fran√ßais / conserver les mots entiers
- BERT fran√ßais -> segmentation tr√®s fine
- BERT multilingue -> flexibilit√© linguistique mais peut sacrifier certaines sp√©cificit√©s

CamemBERT :

- S√©pare "d" et l'apostrophe
- Utilise "‚ñÅ" pour indiquer le d√©but d'un nouveau mot

FlauBERT :

- Garde "d'" ensemble
- Utilise "</w>" pour indiquer la fin d'un mot
  Les traits d'union sont g√©n√©ralement tokenis√©s comme des tokens s√©par√©s ou utilis√©s pour segmenter les mots.
  Indicateurs de sous-mots :

BERT fran√ßais et BERT multilingue :

- Utilisent "##" pour indiquer que le token est une continuation du pr√©c√©dent

```json
{
  "Phrase": "quels trains voyagent d'Alen√ßon √† Corbeil-Essonnes",
  "D√©part": "Alen√ßon",
  "Arriv√©e": "Corbeil-Essonnes",
  "Tokens": {
    "camembert-base": ["‚ñÅquels", "‚ñÅtrains", "‚ñÅvoyage", "nt", "‚ñÅd", "'", "Al", "en", "√ßon", "‚ñÅ√†", "‚ñÅCorb", "e", "il", "-", "Essonne", "s"],
    "flaubert/flaubert_base_cased": ["quels</w>", "trains</w>", "voyagent</w>", "d'</w>", "Alen√ßon</w>", "√†</w>", "Cor", "be", "il-", "Ess", "onnes</w>"],
    "dbmdz/bert-base-french-europeana-cased": ["quels", "trains", "voyage", "##nt", "d", "'", "Alen√ßon", "√†", "Corbeil", "-", "Ess", "##onnes"],
    "bert-base-multilingual-cased": ["quel", "##s", "trains", "voyage", "##nt", "d", "'", "Ale", "##n√ß", "##on", "√†", "Cor", "##bei", "##l", "-", "Essonne", "##s"]
  }
}
```

#### Entrainement d'un mod√®le

üöß chantier est en court  
Pour qu'il puisse predir le d√©part et l'arriv√© avec le sens de la phrase.

## Conclusion

üöß en cours ..  
Le script 3 qui est performant avec une approche simple sans entrainement de mod√®le.
La tokenization avec la lib python NLTK est bas√© sur une REGEX qui prend en compte les contractions du fran√ßais (d'|l'|j'|qu'|n'|s'|t'|m'|c'). Cette m√©thode d√©tecte correctement les entity (D√©part, Arriv√©e) dans une forme naturelle des mots sans les sur-fragmenter en sous-tokens, comme le font les tokenizers de mod√®les pr√©-entra√Æn√©s. De plus le format BIO (Begin, Inside, Outside) est plus lisible pour l'√©valuation humaine.

L'utilisation d'un mod√®le finetun√© üöß en cours ..
