# Recherche pour tokenizer

## But de la recherche

La prediction d'un d√©part et d'une arriv√© dans une phrase.

## install

```
pip install tensorflow pandas numpy transformers
pip install pandas transformers sacremoses nltk
```

## Phrases

üìÑ utilisation du fichier reservation-first-dataset-train.csv

```csv
# ex :
Phrase,D√©part,Arriv√©e
"j'aimerais voyager de Ailly-sur-Somme √† Aix-les-Bains le Revard","Ailly-sur-Somme","Aix-les-Bains le Revard"
"quels trains voyagent d'Alen√ßon √† Corbeil-Essonnes","Alen√ßon","Corbeil-Essonnes"
```

### Script 1

Tokenisation avec `word_tokenize` de `NLTK`  
-> utilise simplement str.split()  
**Probl√®mes**

- Les contractions comme "d'Alen√ßon" ne sont pas g√©r√©es
- Ne trouve pas l'entit√©

```json
   {
    "tokens": ["j'aimerais", "voyager", "de", "Ailly-sur-Somme", "√†", "Aix-les-Bains", "le", "Revard"],
    "labels": ["O", "O", "O", "B-DEP", "O", "B-ARR", "I-ARR", "I-ARR"]
  },
  {
    "tokens": ["quels", "trains", "voyagent", "d'Alen√ßon", "√†", "Corbeil-Essonnes"],
    "labels": ["O", "O", "O", "O", "O", "B-ARR"]
  }
```

### Script 2

Introduction d'un `RegexpTokenizer` personnalis√© pour g√©rer les contractions fran√ßaises  
REGEX : `r"\b(?:d'|l'|j'|qu'|n'|s'|t'|m'|c')?\w+\b"`  
Ce tokenizer tokenise √† la fois les phrases et les mots  
ex: Ailly-sur-Somme est d√©coup√© en 3 tokens

**Probl√®mes**

- Contractions non s√©par√©es (ex: "d'Alen√ßon" est un seul token)
- Erreur d'annotation

```json
  {
    "tokens": ["j'aimerais", "voyager", "de", "Ailly", "sur", "Somme", "√†", "Aix", "les", "Bains", "le", "Revard"],
    "labels": ["O", "O", "O", "B-DEP", "I-DEP", "I-DEP", "O", "B-ARR", "I-ARR", "I-ARR", "I-ARR", "I-ARR"]
  },
  {
    "tokens": ["quels", "trains", "voyagent", "d'Alen√ßon", "√†", "Corbeil", "Essonnes"],
    "labels": ["O", "O", "O", "O", "O", "B-ARR", "I-ARR"]
  }
```

### Script 3

Am√©lioration
REGEX : `r"\b(?:d'|l'|j'|qu'|n'|s'|t'|m'|c')\b|\b\w+\b"`

- S√©pare les contractions en deux tokens distincts ("d'" et "Alen√ßon")

Tr√®s bon r√©sultat

```json
  {
    "tokens": ["j'", "aimerais", "voyager", "de", "Ailly", "sur", "Somme", "√†", "Aix", "les", "Bains", "le", "Revard"],
    "labels": ["O", "O", "O", "O", "B-DEP", "I-DEP", "I-DEP", "O", "B-ARR", "I-ARR", "I-ARR", "I-ARR", "I-ARR"]
  },
  {
    "tokens": ["quels", "trains", "voyagent", "d'", "Alen√ßon", "√†", "Corbeil", "Essonnes"],
    "labels": ["O", "O", "O", "O", "B-DEP", "O", "B-ARR", "I-ARR"]
  }
```

### Script 4

Observer la sortie des tokenizeurs entrain√©s sur du fr avec des lib d'Hugging Face. Le pipeline NER ne peut pas deviner par lui m√™me le d√©part et l'arriv√© sans √™tre entrain√© sur les donn√©es.

Chaque tokenizeur a sa propre strat√©gie de segmentation :

- CamemBERT et FlauBERT -> optimis√©s pour le fran√ßais / conserver les mots entiers
- BERT fran√ßais -> segmentation tr√®s fine
- BERT multilingue -> flexibilit√© linguistique mais peut sacrifier certaines sp√©cificit√©s

CamemBERT :

- S√©pare "d" et l'apostrophe
- Utilise "‚ñÅ" pour indiquer le d√©but d'un nouveau mot

FlauBERT :

- Garde "d'" ensemble
- Utilise "</w>" pour indiquer la fin d'un mot
  Les traits d'union sont g√©n√©ralement tokenis√©s comme des tokens s√©par√©s ou utilis√©s pour segmenter les mots.
  Indicateurs de sous-mots :

BERT fran√ßais et BERT multilingue :

- Utilisent "##" pour indiquer que le token est une continuation du pr√©c√©dent

```json
{
  "Phrase": "quels trains voyagent d'Alen√ßon √† Corbeil-Essonnes",
  "D√©part": "Alen√ßon",
  "Arriv√©e": "Corbeil-Essonnes",
  "Tokens": {
    "camembert-base": ["‚ñÅquels", "‚ñÅtrains", "‚ñÅvoyage", "nt", "‚ñÅd", "'", "Al", "en", "√ßon", "‚ñÅ√†", "‚ñÅCorb", "e", "il", "-", "Essonne", "s"],
    "flaubert/flaubert_base_cased": ["quels</w>", "trains</w>", "voyagent</w>", "d'</w>", "Alen√ßon</w>", "√†</w>", "Cor", "be", "il-", "Ess", "onnes</w>"],
    "dbmdz/bert-base-french-europeana-cased": ["quels", "trains", "voyage", "##nt", "d", "'", "Alen√ßon", "√†", "Corbeil", "-", "Ess", "##onnes"],
    "bert-base-multilingual-cased": ["quel", "##s", "trains", "voyage", "##nt", "d", "'", "Ale", "##n√ß", "##on", "√†", "Cor", "##bei", "##l", "-", "Essonne", "##s"]
  }
}
```

#### Entrainement d'un mod√®le

Le but ici est de faire porter la tokenization et la prediction par un model finetun√© pour notre besoin. On va utiliser BERT et son tokenizer Fast et tester diff√©rents param√©trages d'entrainement sur des donn√©es de train. Puis ensuite, √©valuer le nouveau mod√®le sur des donn√©es de test.

**Configuration**

```markdown
- model de d√©part -> `bert-base-multilingual-cased`
- optimizer avec Adam
- Learning rate 5e-5 => (0.00005) = petite vitesse d'ajustement des poids
- `SparseCategoricalCrossentropy` -> fonction de perte pour le multiclasse
- `from_logits=True` -> "logits" = les sorties du mod√®le non normalis√©s -> avant de calculer la perte, la fonction appliquera une couche softmax interne pour convertir les logits en probabilit√©
- metrics=['accuracy'] pour surveiller la pr√©cision du mod√®le pendant l'entra√Ænement et l'√©valuation

- 3 epochs
- batch_size=16
```

**Resultat**

```sh

Matrice de confusion:
[[ 18   1   1   0   0]
 [  3  18   0   1   0]
 [  0   0  54   2   1]
 [  0   0  11  70   0]
 [  1   1   6   2 530]]

Rapport de classification:
              precision    recall  f1-score   support

           O       0.82      0.90      0.86        20
       B-DEP       0.90      0.82      0.86        22
       I-DEP       0.75      0.95      0.84        57
       B-ARR       0.93      0.86      0.90        81
       I-ARR       1.00      0.98      0.99       540

    accuracy                           0.96       720
   macro avg       0.88      0.90      0.89       720
weighted avg       0.96      0.96      0.96       720

```

**Test de variation des param√®tres d'entrainement**
| batch size + | epoch + | batch size - |
|---|---|---|
| batch_size=36, <br>epoch=3 | batch_size=16, <br>epochs=10 | batch_size=9, <br>epochs=10 |
| ![batch-36.png](img/batch-36.png) | ![10epoch.png](img/10epoch.png) | ![batch-9.png](img/batch-9.png) |

## Conclusion

Le script 3 qui est performant avec une approche simple sans entrainement de mod√®le.
La tokenization avec la lib python NLTK est bas√© sur une REGEX qui prend en compte les contractions du fran√ßais (d'|l'|j'|qu'|n'|s'|t'|m'|c'). Cette m√©thode d√©tecte correctement les entity (D√©part, Arriv√©e) dans une forme naturelle des mots sans les sur-fragmenter en sous-tokens, comme le font les tokenizers de mod√®les pr√©-entra√Æn√©s. De plus le format BIO (Begin, Inside, Outside) est plus lisible pour l'√©valuation humaine.

Le mod√®le finetun√© lui va permettre de faire les prediction sur les donn√©es inconnues.
IL est plus performant avec un plus petit batch.

Enfin, la solution envisag√© pour que le mod√®le soit plus performant serait d'avoir beaucoup plus de donn√©es d'entrainement avec des cas atypiques.
