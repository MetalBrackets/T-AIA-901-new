{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Prepa des data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> loaded successfully\n",
      "Train sentences: 207\n",
      "Test sentences: 20\n",
      "Index(['Phrase', 'Départ', 'Arrivée'], dtype='object')\n",
      "Index(['Phrase', 'Départ', 'Arrivée'], dtype='object')\n",
      "-> Head train data\n",
      "                                              Phrase                Départ  \\\n",
      "0  montrer les trains de Gargan à Valdahon Camp M...                Gargan   \n",
      "1  quels trains sont disponibles de Montbard à Sa...              Montbard   \n",
      "2  j'ai besoin d'un train demain de Saint-Jodard ...          Saint-Jodard   \n",
      "3  montre-moi les trains de Montlouis-sur-Loire à...   Montlouis-sur-Loire   \n",
      "4  tous les trains de Nanteuil-le-Haudouin à Vign...  Nanteuil-le-Haudouin   \n",
      "\n",
      "                   Arrivée  \n",
      "0  Valdahon Camp Militaire  \n",
      "1      Saint-Romain-le-Puy  \n",
      "2                Champigny  \n",
      "3                 Maroeuil  \n",
      "4        Vigneux-sur-Seine  \n",
      "-> Head test data\n",
      "                                              Phrase               Départ  \\\n",
      "0  S'il vous plaît, donnez-moi des trains d'Imphy...                Imphy   \n",
      "1  afficher tous les trains partant lundi matin d...          Chenevières   \n",
      "2        montre-moi les trains de Deluz à Frontignan                Deluz   \n",
      "3  les trains depuis Montpellier-St-Roch pour Dol...  Montpellier-St-Roch   \n",
      "4  je suis intéressé pour aller sur un Ouigo de M...                Melun   \n",
      "\n",
      "           Arrivée  \n",
      "0      Puget-Ville  \n",
      "1    Beaurainville  \n",
      "2       Frontignan  \n",
      "3  Dol-de-Bretagne  \n",
      "4            Auray  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the paths to the CSV files\n",
    "file_path_train = '/home/jovyan/data/reservation-first-dataset-train.csv'\n",
    "file_path_test = '/home/jovyan/data/reservation-first-dataset-test.csv'\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    data_train = pd.read_csv(file_path_train).fillna('')  # Handle possible NaN values\n",
    "    data_test = pd.read_csv(file_path_test).fillna('')    # Handle possible NaN values\n",
    "    print(\"-> loaded successfully\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error : please verify the file paths\")\n",
    "\n",
    "print(f\"Train sentences: {len(data_train)}\")\n",
    "print(f\"Test sentences: {len(data_test)}\")\n",
    "print(data_train.columns)\n",
    "print(data_test.columns)\n",
    "\n",
    "print(\"-> Head train data\")\n",
    "print(data_train.head())\n",
    "print(\"-> Head test data\")\n",
    "print(data_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertTokenizerFast, TFBertForTokenClassification\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) # Masquer les avertissements\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = toutes les logs, 1 = info logs masqués, 2 = info et warning masqués, 3 = tout masqué\n",
    "\n",
    "\n",
    "# Chemins vers les fichiers CSV\n",
    "file_path_train = '/home/jovyan/data/reservation-first-dataset-train.csv'\n",
    "file_path_test = '/home/jovyan/data/reservation-first-dataset-test.csv'\n",
    "\n",
    "# Chargement des données\n",
    "data_train = pd.read_csv(file_path_train).fillna('')\n",
    "data_test = pd.read_csv(file_path_test).fillna('')\n",
    "\n",
    "# Préparer les données pour l'entraînement \n",
    "# Encodage des phrases et des labels correspondants\n",
    "# data = CSV en input / max_length = Longueur maximale des phrases\n",
    "def encode_data(data, tokenizer, label_encoder, max_length=128):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "\n",
    "    print(\"Starting data encoding...\")\n",
    "    for i, row in data.iterrows():\n",
    "        phrase = row['Phrase']\n",
    "        dep = row['Départ']\n",
    "        arr = row['Arrivée']\n",
    "\n",
    "        if i < 5:  # print 5 lignes\n",
    "            print(f\"\\n- Ligne {i+1}\")\n",
    "            print(\"Phrase originale :\", phrase)\n",
    "\n",
    "        tokenized_input = tokenizer.encode_plus(\n",
    "            phrase,\n",
    "            add_special_tokens=True,  # [CLS] au début et [SEP] à la fin\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors=\"tf\",  # retourne les tokens en tensors\n",
    "            max_length=max_length,\n",
    "            truncation=True,  # si la phrase est +longue que max_length\n",
    "            padding=\"max_length\"  # padding pour que ttes les séquences aient la même longueur\n",
    "        )\n",
    "\n",
    "        tokenized_text = tokenizer.convert_ids_to_tokens(tokenized_input.input_ids[0])\n",
    "        offsets = tokenized_input['offset_mapping'].numpy()[0]\n",
    "        label_list = ['O'] * len(tokenized_text)\n",
    "\n",
    "        if i < 5:\n",
    "            print(\"Tokens encodés :\", tokenized_text)\n",
    "            print(\"Offsets :\", offsets)\n",
    "\n",
    "        # Encodage des entités\n",
    "        for j, (start, end) in enumerate(offsets):\n",
    "            if start and end and start != end:\n",
    "                token_str = phrase[start:end]\n",
    "                if token_str in dep:\n",
    "                    label_list[j] = 'B-DEP'\n",
    "                elif token_str in arr:\n",
    "                    label_list[j] = 'B-ARR'\n",
    "\n",
    "        if i < 5:\n",
    "            print(\"Labels après encodage des entités :\", label_list)\n",
    "\n",
    "        # Conversion des labels en ids\n",
    "        label_ids = label_encoder.transform(label_list)\n",
    "        tokens.append(tokenized_input.input_ids.numpy()[0])\n",
    "        labels.append(label_ids)\n",
    "\n",
    "        if i < 5:\n",
    "            print(\"IDs des label\", label_ids)\n",
    "\n",
    "    print(\"Encoding completed\")\n",
    "    return np.array(tokens), np.array(labels)\n",
    "\n",
    "\n",
    "# Version fast du tokenizer BERT\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Préparation des labels\n",
    "unique_labels = ['O', 'B-DEP', 'B-ARR']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(unique_labels)\n",
    "print(\"-> mapping labels et ids :\", {label: idx for idx, label in enumerate(label_encoder.classes_)})\n",
    "\n",
    "# Encoder les données de train et de test\n",
    "train_tokens, train_labels = encode_data(data_train, tokenizer, label_encoder, max_length=128)\n",
    "test_tokens, test_labels = encode_data(data_test, tokenizer, label_encoder, max_length=128)\n",
    "print(\"- shape train tokens\", train_tokens.shape)\n",
    "print(\"- shape trains labels\", train_labels.shape)\n",
    "print(\"- shape test tokens\", test_tokens.shape)\n",
    "print(\"- shape test labels\", test_labels.shape)\n",
    "\n",
    "# Chargement du modèle\n",
    "model = TFBertForTokenClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(label_encoder.classes_))\n",
    "print(\"-> modèle chargé avec\", len(label_encoder.classes_), \"labels\")\n",
    "\n",
    "# Configuration de l'entraînement\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Entraînement\n",
    "model.fit(train_tokens, train_labels, epochs=3, batch_size=16, validation_split=0.1)\n",
    "\n",
    "# Évaluation\n",
    "model.evaluate(test_tokens, test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction et Entraînement du Modèle TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Évaluation du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()\n",
    "\n",
    "# Example function to get predictions\n",
    "def get_predictions(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "    outputs = model(inputs)\n",
    "    predictions = np.argmax(outputs.logits, axis=-1)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].numpy()[0])\n",
    "\n",
    "    # Extract and print the predicted tokens and corresponding entities\n",
    "    return [(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0]) if label_list[prediction] != 'O']\n",
    "\n",
    "# Test with an example sentence\n",
    "example_sentence = \"Je dois voyager de Paris à Lyon\"\n",
    "print(get_predictions(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
