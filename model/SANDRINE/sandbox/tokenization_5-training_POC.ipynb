{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning de BERT -> NER\n",
    "Ce programme a pour but d'entrainer les dernières couches d'un modele pré-entrainé avec des données du dataset de train.    \n",
    "Ainsi, il pourra effectuer une tâche de classification qui consiste à détecter les entités de départ et d'arrivée dans une phrase (un ordre de réservation de voyages) en français."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Observer les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devme\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error : please verify the file paths\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError : please verify the file paths\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain sentences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mdata_train\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest sentences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(data_train\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_train' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import logging as hf_logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# Access to csv\n",
    "file_path_train = '/home/jovyan/data/reservation-first-dataset-train.csv'\n",
    "file_path_test = '/home/jovyan/data/reservation-first-dataset-test.csv'\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    data_train = pd.read_csv(file_path_train).fillna('')  # Handle possible NaN values\n",
    "    data_test = pd.read_csv(file_path_test).fillna('')    # Handle possible NaN values\n",
    "    print(\"-> loaded successfully\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error : please verify the file paths\")\n",
    "\n",
    "print(f\"Train sentences: {len(data_train)}\")\n",
    "print(f\"Test sentences: {len(data_test)}\")\n",
    "print(data_train.columns)\n",
    "print(data_test.columns)\n",
    "\n",
    "print(\"-> Head train data\")\n",
    "print(data_train.head())\n",
    "print(\"-> Head test data\")\n",
    "print(data_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Tokenizer et encoder - First attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertTokenizerFast, TFBertForTokenClassification\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0' # 0 = all logs, 1 = info logs hidden, 2 = info and warning hidden, 3 = all hidden\n",
    "\n",
    "file_path_train = '/home/jovyan/data/reservation-first-dataset-train.csv'\n",
    "file_path_test = '/home/jovyan/data/reservation-first-dataset-test.csv'\n",
    "data_train = pd.read_csv(file_path_train).fillna('')\n",
    "data_test = pd.read_csv(file_path_test).fillna('')\n",
    "\n",
    "# Encoding of sentences and corresponding labels\n",
    "def encode_data(data, tokenizer, label_encoder, max_length=36): # data = CSV input / max_length = max token in a sentence\n",
    "    tokens = []\n",
    "    labels = []\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        phrase = row['Phrase']\n",
    "        dep = row['Départ']\n",
    "        arr = row['Arrivée']\n",
    "\n",
    "        if i < 4:  # print 4 lignes\n",
    "            print(f\"\\n---> Ligne {i+1}\")\n",
    "            print(\"- Original sentence: \", phrase)\n",
    "\n",
    "        tokenized_input = tokenizer.encode_plus(\n",
    "            phrase,\n",
    "            add_special_tokens=True,  # [CLS] at the beginning and [SEP] at the end\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors=\"tf\",  # returns tokens to tensors\n",
    "            max_length=max_length,\n",
    "            truncation=True,  # if the sentence is longer than max_length\n",
    "            padding=\"max_length\"  # so that all sequences have the same number of tokens\n",
    "        )\n",
    "\n",
    "        tokenized_text = tokenizer.convert_ids_to_tokens(tokenized_input.input_ids[0])\n",
    "        offsets = tokenized_input['offset_mapping'].numpy()[0]\n",
    "        label_list = ['O'] * len(tokenized_text)\n",
    "\n",
    "        if i < 4:\n",
    "            print(\"- Encoded tokens: \", tokenized_text)\n",
    "            # print(\"Offsets :\", offsets)\n",
    "\n",
    "        for j, (start, end) in enumerate(offsets):\n",
    "            if start and end and start != end:\n",
    "                token_str = phrase[start:end]\n",
    "                if token_str in dep:\n",
    "                    label_list[j] = 'B-DEP'\n",
    "                elif token_str in arr:\n",
    "                    label_list[j] = 'B-ARR'\n",
    "\n",
    "        if i < 4:\n",
    "            print(\"- Labels after encoding the entities: \", label_list)\n",
    "\n",
    "        # Converting labels to ids\n",
    "        label_ids = label_encoder.transform(label_list)\n",
    "        tokens.append(tokenized_input.input_ids.numpy()[0])\n",
    "        labels.append(label_ids)\n",
    "\n",
    "        if i < 4:\n",
    "            print(\"- Label IDs \", label_ids)\n",
    "\n",
    "    print(\"\\n// Encoding completed\")\n",
    "    return np.array(tokens), np.array(labels)\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased') # Fast version of the BERT tokenizer\n",
    "\n",
    "print(\"\\n// Preparation of labels ['O', 'B-DEP', 'B-ARR']\")\n",
    "unique_labels = ['O', 'B-DEP', 'B-ARR']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(unique_labels)\n",
    "print(\"-> mapping labels and ids: \", {label: idx for idx, label in enumerate(label_encoder.classes_)})\n",
    "\n",
    "print(\"\\n// Starting train data encoding...\")\n",
    "train_tokens, train_labels = encode_data(data_train, tokenizer, label_encoder, max_length=36)\n",
    "print(\"\\n// Starting test data encoding...\")\n",
    "test_tokens, test_labels = encode_data(data_test, tokenizer, label_encoder, max_length=36)\n",
    "\n",
    "print(\"\\n// Shape\")\n",
    "print(\"- train tokens\", train_tokens.shape)\n",
    "print(\"- train labels\", train_labels.shape)\n",
    "print(\"- test tokens\", test_tokens.shape)\n",
    "print(\"- test labels\", test_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Analyse First attempt\n",
    "\n",
    "Ici on peut voir que les tokens et les labels sont désalignés à cause de la fonction encode_data.   \n",
    "Elle vérifie si le token_str est contenu dans le nom de la ville de départ ou d'arrivée avec l'opérateur \"in\".   \n",
    "Par ex que le tiret '-' peut être présent dans un nom de ville mais également à n'importe quel endroit du texte.   \n",
    "\n",
    "Pour résoudre ce problème, la proposition est de trouver la position dans la phrase avec les ids.   \n",
    "Et ainsi de vérifier pour chaque token si son offset chevauche l'une des entités. Si oui, le label approprié sera attribué.   \n",
    "\n",
    "Enfin le schéma de labellisation 'BIO' (Begin, Inside, Outside) sera implémenter pour mieux gérer les multi-tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, BertTokenizerFast, TFBertForTokenClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import logging as hf_logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "file_path_train = '/home/jovyan/data/reservation-first-dataset-train.csv'\n",
    "file_path_test = '/home/jovyan/data/reservation-first-dataset-test.csv'\n",
    "data_train = pd.read_csv(file_path_train).fillna('')\n",
    "data_test = pd.read_csv(file_path_test).fillna('')\n",
    "\n",
    "print(\"\\n// Preparation of labels ['O', 'B-DEP', 'I-DEP', 'B-ARR', 'I-ARR']\")\n",
    "unique_labels = ['O', 'B-DEP', 'I-DEP', 'B-ARR', 'I-ARR']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(unique_labels)\n",
    "print(\"-> mapping labels and ids: \", {label: idx for idx, label in enumerate(label_encoder.classes_)})\n",
    "\n",
    "# Encoding of sentences and corresponding labels\n",
    "def encode_data(data, tokenizer, label_encoder, max_length=36): # data = CSV input / max_length = max token in a sentence\n",
    "    tokens = []\n",
    "    labels = []\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        phrase = row['Phrase']\n",
    "        dep = row['Départ']\n",
    "        arr = row['Arrivée']\n",
    "\n",
    "        if i < 4:  # print 4 lignes\n",
    "            print(f\"\\n---> Ligne {i+1}\")\n",
    "            print(\"- Original sentence: \", phrase)\n",
    "\n",
    "        dep_positions = []\n",
    "        arr_positions = []\n",
    "\n",
    "        start = 0  # Find all starting occurrences\n",
    "        while True:\n",
    "            idx = phrase.find(dep, start)\n",
    "            if idx == -1:\n",
    "                break\n",
    "            dep_positions.append((idx, idx + len(dep)))\n",
    "            start = idx + len(dep)\n",
    "\n",
    "        start = 0  # Find all arrival occurrences\n",
    "        while True:\n",
    "            idx = phrase.find(arr, start)\n",
    "            if idx == -1:\n",
    "                break\n",
    "            arr_positions.append((idx, idx + len(arr)))\n",
    "            start = idx + len(arr)\n",
    "\n",
    "        tokenized_input = tokenizer.encode_plus(\n",
    "            phrase, add_special_tokens=True, return_offsets_mapping=True, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"tf\"\n",
    "        )\n",
    "\n",
    "        tokenized_text = tokenizer.convert_ids_to_tokens(tokenized_input.input_ids[0])\n",
    "        offsets = tokenized_input['offset_mapping'].numpy()[0]\n",
    "        label_list = ['O'] * len(tokenized_text)\n",
    "\n",
    "        if i < 4:\n",
    "            print(\"- Encoded tokens: \", tokenized_text)\n",
    "\n",
    "        for j, (offset_start, offset_end) in enumerate(offsets): # Assignment of labels to tokens\n",
    "            if offset_start == 0 and offset_end == 0:\n",
    "                continue  # Token of padding\n",
    "            token_label = 'O'\n",
    "\n",
    "            # Check if the token overlaps a starting entity\n",
    "            for entity_start, entity_end in dep_positions:\n",
    "                if (offset_start >= entity_start) and (offset_end <= entity_end):\n",
    "                    if offset_start == entity_start:\n",
    "                        token_label = 'B-DEP'\n",
    "                    else:\n",
    "                        token_label = 'I-DEP'\n",
    "                    break\n",
    "\n",
    "            # Check if the token overlaps an arrival entity\n",
    "            for entity_start, entity_end in arr_positions:\n",
    "                if (offset_start >= entity_start) and (offset_end <= entity_end):\n",
    "                    if offset_start == entity_start:\n",
    "                        token_label = 'B-ARR'\n",
    "                    else:\n",
    "                        token_label = 'I-ARR'\n",
    "                    break\n",
    "\n",
    "            label_list[j] = token_label\n",
    "\n",
    "        if i < 4:\n",
    "            print(\"- Labels after encoding the entities: \", label_list)\n",
    "\n",
    "        label_ids = label_encoder.transform(label_list)\n",
    "        tokens.append(tokenized_input.input_ids.numpy()[0])\n",
    "        labels.append(label_ids)\n",
    "\n",
    "        if i < 4:\n",
    "            print(\"- Label IDs\", label_ids)\n",
    "\n",
    "    print(\"\\n// Encoding completed\")\n",
    "    return np.array(tokens), np.array(labels)\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased') # Fast version of the BERT tokenizer\n",
    "\n",
    "print(\"\\n// Starting train data encoding...\")\n",
    "train_tokens, train_labels = encode_data(data_train, tokenizer, label_encoder, max_length=36)\n",
    "print(\"\\n// Starting test data encoding...\")\n",
    "test_tokens, test_labels = encode_data(data_test, tokenizer, label_encoder, max_length=36)\n",
    "\n",
    "# Model training\n",
    "model = TFBertForTokenClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(label_encoder.classes_)) # loading\n",
    "print(\"-> model loaded with \", len(label_encoder.classes_), \"labels\")\n",
    "\n",
    "# configuration\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_tokens, train_labels, epochs=10, batch_size=9, validation_split=0.1) # training\n",
    "\n",
    "# Save\n",
    "model_path = os.path.join(os.getcwd(), '..', '..', 'model', 'models', 'bert-base-multilingual-cased')\n",
    "absolute_model_path = os.path.abspath(model_path)\n",
    "print(f\"Path : {absolute_model_path}\")\n",
    "\n",
    "model.save_pretrained(absolute_model_path)  # model\n",
    "tokenizer.save_pretrained(absolute_model_path)  # tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Évaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model evaluation\n",
    "evaluation_results = model.evaluate(test_tokens, test_labels)\n",
    "print(\"Evaluation results:\", evaluation_results)\n",
    "\n",
    "test_predictions = model.predict(test_tokens).logits\n",
    "predicted_labels = np.argmax(test_predictions, axis=-1).flatten()\n",
    "\n",
    "true_labels = test_labels.flatten() \n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # le nb de vrais labels pour chaque classe\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "custom_palette = [\"#DCDED6\", \"#CED0C3\", \"#B4BAB1\", \"#859393\", \"#5D726F\", \"#485665\"]\n",
    "ax = sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap=custom_palette, xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, square=True, linewidths=2, linecolor='white')\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.xticks(fontsize=10, fontweight='normal')\n",
    "plt.yticks(fontsize=10, fontweight='normal')\n",
    "\n",
    "# color based on value\n",
    "for text in ax.texts:\n",
    "    t = float(text.get_text())\n",
    "    if 0.1 <= t < 0.7:\n",
    "        text.set_color('#CA3C66')\n",
    "    elif t >= 0.7:\n",
    "        text.set_color('#FFFFFF')\n",
    "    else:\n",
    "        text.set_color('#485665')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix stdout\n",
    "confusion = confusion_matrix(true_labels, predicted_labels)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion)\n",
    "\n",
    "# Classification Report stdout\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels, target_names=unique_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Prédiction et évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "unique_labels = ['O', 'B-DEP', 'I-DEP', 'B-ARR', 'I-ARR']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(unique_labels)\n",
    "\n",
    "new_phrase = \"demain, je planifie un voyage de Saint-Germain-en-Laye à L'Haÿ-les-Roses, en passant prendre des amis à Asnières-sur-Seine et peut-être faire une halte à Vitry-sur-Seine pour le déjeuner\"\n",
    "\n",
    "# Encodde data with correspondant labels\n",
    "encoded_data = encode_data(pd.DataFrame([{'Phrase': new_phrase, 'Départ': 'Saint-Germain-en-Laye', 'Arrivée': \"L'Haÿ-les-Roses\"}]), tokenizer, label_encoder, max_length=36)\n",
    "new_tokens, new_labels = encoded_data\n",
    "\n",
    "# Model prediction\n",
    "new_predictions = model.predict(new_tokens).logits\n",
    "new_predicted_labels = np.argmax(new_predictions, axis=-1)\n",
    "\n",
    "# Convert ids to text label\n",
    "predicted_label_names = [label_encoder.inverse_transform([label_id])[0] for label_id in new_predicted_labels[0]]\n",
    "\n",
    "# Get true label\n",
    "true_labels = [label_encoder.inverse_transform([label_id])[0] for label_id in new_labels[0]]\n",
    "\n",
    "# Check success\n",
    "prediction_success = predicted_label_names == true_labels\n",
    "print(\"\\nPrediction success:\", prediction_success)\n",
    "\n",
    "# Result\n",
    "tokens_with_special_tokens = tokenizer.convert_ids_to_tokens(new_tokens[0])\n",
    "\n",
    "print(\"\\nResult:\")\n",
    "print(\"{:<14} {:<10} {:<8} {:<6}\".format(\"Token\", \"Predicted\", \"True\", \"Result\"))\n",
    "print(\"-\" * 42)\n",
    "\n",
    "for token, predicted_label, true_label in zip(tokens_with_special_tokens, predicted_label_names, true_labels):\n",
    "    result = \"✔️\" if predicted_label == true_label else \"❌\"\n",
    "    print(\"{:<14} {:<10} {:<8} {:<6}\".format(token, predicted_label, true_label, result))\n",
    "\n",
    "# Classification Report stdout\n",
    "print(\"\\nDetailed classification report:\")\n",
    "print(classification_report(true_labels, predicted_label_names, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "On a un modèle dans l'ensemble assez performant qui a une accuracy moyenne de 97% à l'évaluation de tous labels confondu, sur les données de test.   \n",
    "Et on voit qu'il s'en sort bien sur une tâche de prediction avec une phrase complexe en entrée.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
