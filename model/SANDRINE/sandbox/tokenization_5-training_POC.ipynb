{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning de BERT -> NER\n",
    "Ce programme a pour but d'entrainer les dernières couches d'un modele pré-entrainé avec des données provenant d'un dataset transformé.    \n",
    "Ainsi, il pourra effectuer une tâche de classification qui consiste à détecter les entités de départ et d'arrivée dans une phrase (un ordre de réservation de voyages) en français."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Observer les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> loaded successfully\n",
      "Train sentences: 207\n",
      "Test sentences: 20\n",
      "Index(['Phrase', 'Départ', 'Arrivée'], dtype='object')\n",
      "Index(['Phrase', 'Départ', 'Arrivée'], dtype='object')\n",
      "-> Head train data\n",
      "                                              Phrase                 Départ  \\\n",
      "0  montre-moi les trains dimanche allant de Jarvi...  Jarville-la-Malgrange   \n",
      "1  quels trains voyagent d'Alençon à Corbeil-Esso...                Alençon   \n",
      "2  montre-moi les trains pour Saint-Avold depuis ...               Xertigny   \n",
      "3  montrer les trains de Gargan à Valdahon Camp M...                 Gargan   \n",
      "4  quels trains sont disponibles de Montbard à Sa...               Montbard   \n",
      "\n",
      "                   Arrivée  \n",
      "0      La Bassée-Violaines  \n",
      "1         Corbeil-Essonnes  \n",
      "2              Saint-Avold  \n",
      "3  Valdahon Camp Militaire  \n",
      "4      Saint-Romain-le-Puy  \n",
      "-> Head test data\n",
      "                                              Phrase               Départ  \\\n",
      "0  S'il vous plaît, donnez-moi des trains d'Imphy...                Imphy   \n",
      "1  afficher tous les trains partant lundi matin d...          Chenevières   \n",
      "2        montre-moi les trains de Deluz à Frontignan                Deluz   \n",
      "3  les trains depuis Montpellier-St-Roch pour Dol...  Montpellier-St-Roch   \n",
      "4  je suis intéressé pour aller sur un Ouigo de M...                Melun   \n",
      "\n",
      "           Arrivée  \n",
      "0      Puget-Ville  \n",
      "1    Beaurainville  \n",
      "2       Frontignan  \n",
      "3  Dol-de-Bretagne  \n",
      "4            Auray  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the paths to the CSV files\n",
    "file_path_train = '/home/jovyan/data/reservation-first-dataset-train.csv'\n",
    "file_path_test = '/home/jovyan/data/reservation-first-dataset-test.csv'\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    data_train = pd.read_csv(file_path_train).fillna('')  # Handle possible NaN values\n",
    "    data_test = pd.read_csv(file_path_test).fillna('')    # Handle possible NaN values\n",
    "    print(\"-> loaded successfully\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error : please verify the file paths\")\n",
    "\n",
    "print(f\"Train sentences: {len(data_train)}\")\n",
    "print(f\"Test sentences: {len(data_test)}\")\n",
    "print(data_train.columns)\n",
    "print(data_test.columns)\n",
    "\n",
    "print(\"-> Head train data\")\n",
    "print(data_train.head())\n",
    "print(\"-> Head test data\")\n",
    "print(data_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Tokenizer et encoder - First essaie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "// Preparation of labels ['O', 'B-DEP', 'B-ARR']\n",
      "-> mapping labels et ids : {'B-ARR': 0, 'B-DEP': 1, 'O': 2}\n",
      "\n",
      "// Starting train data encoding...\n",
      "\n",
      "- Ligne 1\n",
      "Phrase originale : montre-moi les trains dimanche allant de Jarville-la-Malgrange à La Bassée-Violaines en première classe sans correspondance partant l'après midi\n",
      "Tokens encodés : ['[CLS]', 'montre', '-', 'moi', 'les', 'trains', 'dimanche', 'allant', 'de', 'Jar', '##ville', '-', 'la', '-', 'Mal', '##gra', '##nge', 'à', 'La', 'Bass', '##ée', '-', 'Viola', '##ines', 'en', 'première', 'classe', 'sans', 'correspond', '##ance', 'part', '##ant', 'l', \"'\", 'après', '[SEP]']\n",
      "Labels après encodage des entités : ['O', 'O', 'B-DEP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DEP', 'B-DEP', 'B-DEP', 'B-DEP', 'B-DEP', 'B-DEP', 'B-DEP', 'B-DEP', 'O', 'B-ARR', 'B-ARR', 'B-ARR', 'B-DEP', 'B-ARR', 'B-ARR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DEP', 'O', 'O', 'O']\n",
      "IDs des label [2 2 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 2 0 0 0 1 0 0 2 2 2 2 2 2 2 2 1 2 2 2]\n",
      "\n",
      "- Ligne 2\n",
      "Phrase originale : quels trains voyagent d'Alençon à Corbeil-Essonnes\n",
      "Tokens encodés : ['[CLS]', 'quel', '##s', 'trains', 'voyage', '##nt', 'd', \"'\", 'Ale', '##nç', '##on', 'à', 'Cor', '##bei', '##l', '-', 'Essonne', '##s', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Labels après encodage des entités : ['O', 'O', 'B-ARR', 'O', 'O', 'O', 'O', 'O', 'B-DEP', 'B-DEP', 'B-DEP', 'O', 'B-ARR', 'B-ARR', 'B-DEP', 'B-ARR', 'B-ARR', 'B-ARR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "IDs des label [2 2 0 2 2 2 2 2 1 1 1 2 0 0 1 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "- Ligne 3\n",
      "Phrase originale : montre-moi les trains pour Saint-Avold depuis Xertigny et s'arrêtant à Boucoiran-et-Nozières\n",
      "Tokens encodés : ['[CLS]', 'montre', '-', 'moi', 'les', 'trains', 'pour', 'Saint', '-', 'Av', '##old', 'depuis', 'X', '##erti', '##gny', 'et', 's', \"'\", 'arrêt', '##ant', 'à', 'Bou', '##co', '##iran', '-', 'et', '-', 'No', '##zi', '##ères', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Labels après encodage des entités : ['O', 'O', 'B-ARR', 'O', 'O', 'O', 'O', 'B-ARR', 'B-ARR', 'B-ARR', 'B-ARR', 'O', 'B-DEP', 'B-DEP', 'B-DEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARR', 'O', 'B-ARR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "IDs des label [2 2 0 2 2 2 2 0 0 0 0 2 1 1 1 2 2 2 2 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "- Ligne 4\n",
      "Phrase originale : montrer les trains de Gargan à Valdahon Camp Militaire entre 18h et 20h le vendredi\n",
      "Tokens encodés : ['[CLS]', 'montrer', 'les', 'trains', 'de', 'Ga', '##rgan', 'à', 'Val', '##dah', '##on', 'Camp', 'Mil', '##itaire', 'entre', '18', '##h', 'et', '20', '##h', 'le', 'vendre', '##di', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Labels après encodage des entités : ['O', 'O', 'O', 'O', 'O', 'B-DEP', 'B-DEP', 'O', 'B-ARR', 'B-ARR', 'B-ARR', 'B-ARR', 'B-ARR', 'B-ARR', 'O', 'O', 'B-ARR', 'O', 'O', 'B-ARR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "IDs des label [2 2 2 2 2 1 1 2 0 0 0 0 0 0 2 2 0 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "// Encoding completed\n",
      "\n",
      "// Starting test data encoding...\n",
      "\n",
      "- Ligne 1\n",
      "Phrase originale : S'il vous plaît, donnez-moi des trains d'Imphy à Puget-Ville le mercredi après-midi et le jeudi matin\n",
      "Tokens encodés : ['[CLS]', 'S', \"'\", 'il', 'vous', 'pla', '##ît', ',', 'donne', '##z', '-', 'moi', 'des', 'trains', 'd', \"'\", 'Im', '##ph', '##y', 'à', 'P', '##uge', '##t', '-', 'Ville', 'le', 'mer', '##cre', '##di', 'après', '-', 'midi', 'et', 'le', 'jeu', '[SEP]']\n",
      "Labels après encodage des entités : ['O', 'O', 'O', 'B-ARR', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARR', 'O', 'O', 'O', 'O', 'O', 'B-DEP', 'B-DEP', 'B-DEP', 'O', 'B-ARR', 'B-ARR', 'B-ARR', 'B-ARR', 'B-ARR', 'B-ARR', 'O', 'O', 'O', 'O', 'B-ARR', 'O', 'B-ARR', 'B-ARR', 'O', 'O']\n",
      "IDs des label [2 2 2 0 2 2 2 2 2 2 0 2 2 2 2 2 1 1 1 2 0 0 0 0 0 0 2 2 2 2 0 2 0 0 2 2]\n",
      "\n",
      "- Ligne 2\n",
      "Phrase originale : afficher tous les trains partant lundi matin de Chenevières à Beaurainville\n",
      "Tokens encodés : ['[CLS]', 'affiche', '##r', 'tous', 'les', 'trains', 'part', '##ant', 'lu', '##ndi', 'matin', 'de', 'Chen', '##evi', '##ères', 'à', 'Beau', '##rain', '##ville', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Labels après encodage des entités : ['O', 'O', 'B-DEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DEP', 'B-DEP', 'B-DEP', 'O', 'B-ARR', 'B-ARR', 'B-ARR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "IDs des label [2 2 1 2 2 2 2 2 2 2 2 2 1 1 1 2 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "- Ligne 3\n",
      "Phrase originale : montre-moi les trains de Deluz à Frontignan\n",
      "Tokens encodés : ['[CLS]', 'montre', '-', 'moi', 'les', 'trains', 'de', 'Del', '##uz', 'à', 'Front', '##ign', '##an', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Labels après encodage des entités : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DEP', 'B-DEP', 'O', 'B-ARR', 'B-ARR', 'B-ARR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "IDs des label [2 2 2 2 2 2 2 1 1 2 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "- Ligne 4\n",
      "Phrase originale : les trains depuis Montpellier-St-Roch pour Dol-de-Bretagne\n",
      "Tokens encodés : ['[CLS]', 'les', 'trains', 'depuis', 'Montpellier', '-', 'St', '-', 'Roc', '##h', 'pour', 'Dol', '-', 'de', '-', 'Bretagne', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Labels après encodage des entités : ['O', 'O', 'O', 'O', 'B-DEP', 'B-DEP', 'B-DEP', 'B-DEP', 'B-DEP', 'B-DEP', 'O', 'B-ARR', 'B-DEP', 'B-ARR', 'B-DEP', 'B-ARR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "IDs des label [2 2 2 2 1 1 1 1 1 1 2 0 1 0 1 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "// Encoding completed\n",
      "\n",
      "// Shape\n",
      "- train tokens (207, 36)\n",
      "- train labels (207, 36)\n",
      "- test tokens (20, 36)\n",
      "- test labels (20, 36)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertTokenizerFast, TFBertForTokenClassification\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) # Masquer les avertissements\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # 0 = toutes les logs, 1 = info logs masqués, 2 = info et warning masqués, 3 = tout masqué\n",
    "\n",
    "file_path_train = '/home/jovyan/data/reservation-first-dataset-train.csv'\n",
    "file_path_test = '/home/jovyan/data/reservation-first-dataset-test.csv'\n",
    "data_train = pd.read_csv(file_path_train).fillna('')\n",
    "data_test = pd.read_csv(file_path_test).fillna('')\n",
    "\n",
    "# Encodage des phrases et des labels correspondants\n",
    "def encode_data(data, tokenizer, label_encoder, max_length=36): # data = CSV en input / max_length = Longueur maximale des phrases\n",
    "    tokens = []\n",
    "    labels = []\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        phrase = row['Phrase']\n",
    "        dep = row['Départ']\n",
    "        arr = row['Arrivée']\n",
    "\n",
    "        if i < 4:  # print 4 lignes\n",
    "            print(f\"\\n- Ligne {i+1}\")\n",
    "            print(\"Phrase originale :\", phrase)\n",
    "\n",
    "        tokenized_input = tokenizer.encode_plus(\n",
    "            phrase,\n",
    "            add_special_tokens=True,  # [CLS] au début et [SEP] à la fin\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors=\"tf\",  # retourne les tokens en tensors\n",
    "            max_length=max_length,\n",
    "            truncation=True,  # si la phrase est +longue que max_length\n",
    "            padding=\"max_length\"  # padding pour que ttes les séquences aient la même longueur\n",
    "        )\n",
    "\n",
    "        tokenized_text = tokenizer.convert_ids_to_tokens(tokenized_input.input_ids[0])\n",
    "        offsets = tokenized_input['offset_mapping'].numpy()[0]\n",
    "        label_list = ['O'] * len(tokenized_text)\n",
    "\n",
    "        if i < 4:\n",
    "            print(\"Tokens encodés :\", tokenized_text)\n",
    "            # print(\"Offsets :\", offsets)\n",
    "\n",
    "        for j, (start, end) in enumerate(offsets):\n",
    "            if start and end and start != end:\n",
    "                token_str = phrase[start:end]\n",
    "                if token_str in dep:\n",
    "                    label_list[j] = 'B-DEP'\n",
    "                elif token_str in arr:\n",
    "                    label_list[j] = 'B-ARR'\n",
    "\n",
    "        if i < 4:\n",
    "            print(\"Labels après encodage des entités :\", label_list)\n",
    "\n",
    "        # Conversion des labels en ids\n",
    "        label_ids = label_encoder.transform(label_list)\n",
    "        tokens.append(tokenized_input.input_ids.numpy()[0])\n",
    "        labels.append(label_ids)\n",
    "\n",
    "        if i < 4:\n",
    "            print(\"IDs des label\", label_ids)\n",
    "\n",
    "    print(\"\\n// Encoding completed\")\n",
    "    return np.array(tokens), np.array(labels)\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased') # Version fast du tokenizer BERT\n",
    "\n",
    "print(\"\\n// Preparation of labels ['O', 'B-DEP', 'B-ARR']\")\n",
    "unique_labels = ['O', 'B-DEP', 'B-ARR']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(unique_labels)\n",
    "print(\"-> mapping labels et ids :\", {label: idx for idx, label in enumerate(label_encoder.classes_)})\n",
    "\n",
    "print(\"\\n// Starting train data encoding...\")\n",
    "train_tokens, train_labels = encode_data(data_train, tokenizer, label_encoder, max_length=36)\n",
    "print(\"\\n// Starting test data encoding...\")\n",
    "test_tokens, test_labels = encode_data(data_test, tokenizer, label_encoder, max_length=36)\n",
    "\n",
    "print(\"\\n// Shape\")\n",
    "print(\"- train tokens\", train_tokens.shape)\n",
    "print(\"- train labels\", train_labels.shape)\n",
    "print(\"- test tokens\", test_tokens.shape)\n",
    "print(\"- test labels\", test_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Analyse First essaie\n",
    "\n",
    "Ici on peut voir que les tokens et les labels sont désalignés à cause de la fonction encode_data.   \n",
    "Elle vérifie si le token_str est contenu dans le nom de la ville de départ ou d'arrivée avec l'opérateur \"in\".   \n",
    "Par ex que le tiret '-' peut être présent dans un nom de ville mais également à n'importe quel endroit du texte.   \n",
    "\n",
    "Pour résoudre ce problème, la proposition est de trouver la position dans la phrase avec les ids.   \n",
    "Et ainsi de vérifier pour chaque token si son offset chevauche l'une des entités. Si oui, le label approprié sera attribué.   \n",
    "\n",
    "Enfin le schéma de labellisation 'BIO' (Begin, Inside, Outside) sera implémenter pour mieux gérer les multi-tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "// Preparation of labels ['O', 'B-DEP', 'I-DEP', 'B-ARR', 'I-ARR']\n",
      "-> mapping labels et ids : {'B-ARR': 0, 'B-DEP': 1, 'I-ARR': 2, 'I-DEP': 3, 'O': 4}\n",
      "\n",
      "// Starting train data encoding...\n",
      "Starting data encoding...\n",
      "\n",
      "- Ligne 1\n",
      "Phrase originale : montre-moi les trains dimanche allant de Jarville-la-Malgrange à La Bassée-Violaines en première classe sans correspondance partant l'après midi\n",
      "Tokens encodés : ['[CLS]', 'montre', '-', 'moi', 'les', 'trains', 'dimanche', 'allant', 'de', 'Jar', '##ville', '-', 'la', '-', 'Mal', '##gra', '##nge', 'à', 'La', 'Bass', '##ée', '-', 'Viola', '##ines', 'en', 'première', 'classe', 'sans', 'correspond', '##ance', 'part', '##ant', 'l', \"'\", 'après', '[SEP]']\n",
      "Labels après encodage des entités : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DEP', 'I-DEP', 'I-DEP', 'I-DEP', 'I-DEP', 'I-DEP', 'I-DEP', 'I-DEP', 'O', 'B-ARR', 'I-ARR', 'I-ARR', 'I-ARR', 'I-ARR', 'I-ARR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "IDs des labels : [4 4 4 4 4 4 4 4 4 1 3 3 3 3 3 3 3 4 0 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "- Ligne 2\n",
      "Phrase originale : quels trains voyagent d'Alençon à Corbeil-Essonnes\n",
      "Tokens encodés : ['[CLS]', 'quel', '##s', 'trains', 'voyage', '##nt', 'd', \"'\", 'Ale', '##nç', '##on', 'à', 'Cor', '##bei', '##l', '-', 'Essonne', '##s', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Labels après encodage des entités : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DEP', 'I-DEP', 'I-DEP', 'O', 'B-ARR', 'I-ARR', 'I-ARR', 'I-ARR', 'I-ARR', 'I-ARR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "IDs des labels : [4 4 4 4 4 4 4 4 1 3 3 4 0 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "- Ligne 3\n",
      "Phrase originale : montre-moi les trains pour Saint-Avold depuis Xertigny et s'arrêtant à Boucoiran-et-Nozières\n",
      "Tokens encodés : ['[CLS]', 'montre', '-', 'moi', 'les', 'trains', 'pour', 'Saint', '-', 'Av', '##old', 'depuis', 'X', '##erti', '##gny', 'et', 's', \"'\", 'arrêt', '##ant', 'à', 'Bou', '##co', '##iran', '-', 'et', '-', 'No', '##zi', '##ères', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Labels après encodage des entités : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARR', 'I-ARR', 'I-ARR', 'I-ARR', 'O', 'B-DEP', 'I-DEP', 'I-DEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "IDs des labels : [4 4 4 4 4 4 4 0 2 2 2 4 1 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "- Ligne 4\n",
      "Phrase originale : montrer les trains de Gargan à Valdahon Camp Militaire entre 18h et 20h le vendredi\n",
      "Tokens encodés : ['[CLS]', 'montrer', 'les', 'trains', 'de', 'Ga', '##rgan', 'à', 'Val', '##dah', '##on', 'Camp', 'Mil', '##itaire', 'entre', '18', '##h', 'et', '20', '##h', 'le', 'vendre', '##di', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Labels après encodage des entités : ['O', 'O', 'O', 'O', 'O', 'B-DEP', 'I-DEP', 'O', 'B-ARR', 'I-ARR', 'I-ARR', 'I-ARR', 'I-ARR', 'I-ARR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "IDs des labels : [4 4 4 4 4 1 3 4 0 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "// Encoding completed\n",
      "\n",
      "// Starting test data encoding...\n",
      "Starting data encoding...\n",
      "\n",
      "- Ligne 1\n",
      "Phrase originale : S'il vous plaît, donnez-moi des trains d'Imphy à Puget-Ville le mercredi après-midi et le jeudi matin\n",
      "Tokens encodés : ['[CLS]', 'S', \"'\", 'il', 'vous', 'pla', '##ît', ',', 'donne', '##z', '-', 'moi', 'des', 'trains', 'd', \"'\", 'Im', '##ph', '##y', 'à', 'P', '##uge', '##t', '-', 'Ville', 'le', 'mer', '##cre', '##di', 'après', '-', 'midi', 'et', 'le', 'jeu', '[SEP]']\n",
      "Labels après encodage des entités : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DEP', 'I-DEP', 'I-DEP', 'O', 'B-ARR', 'I-ARR', 'I-ARR', 'I-ARR', 'I-ARR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "IDs des labels : [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 3 3 4 0 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "- Ligne 2\n",
      "Phrase originale : afficher tous les trains partant lundi matin de Chenevières à Beaurainville\n",
      "Tokens encodés : ['[CLS]', 'affiche', '##r', 'tous', 'les', 'trains', 'part', '##ant', 'lu', '##ndi', 'matin', 'de', 'Chen', '##evi', '##ères', 'à', 'Beau', '##rain', '##ville', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Labels après encodage des entités : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DEP', 'I-DEP', 'I-DEP', 'O', 'B-ARR', 'I-ARR', 'I-ARR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "IDs des labels : [4 4 4 4 4 4 4 4 4 4 4 4 1 3 3 4 0 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "- Ligne 3\n",
      "Phrase originale : montre-moi les trains de Deluz à Frontignan\n",
      "Tokens encodés : ['[CLS]', 'montre', '-', 'moi', 'les', 'trains', 'de', 'Del', '##uz', 'à', 'Front', '##ign', '##an', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Labels après encodage des entités : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DEP', 'I-DEP', 'O', 'B-ARR', 'I-ARR', 'I-ARR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "IDs des labels : [4 4 4 4 4 4 4 1 3 4 0 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "- Ligne 4\n",
      "Phrase originale : les trains depuis Montpellier-St-Roch pour Dol-de-Bretagne\n",
      "Tokens encodés : ['[CLS]', 'les', 'trains', 'depuis', 'Montpellier', '-', 'St', '-', 'Roc', '##h', 'pour', 'Dol', '-', 'de', '-', 'Bretagne', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Labels après encodage des entités : ['O', 'O', 'O', 'O', 'B-DEP', 'I-DEP', 'I-DEP', 'I-DEP', 'I-DEP', 'I-DEP', 'O', 'B-ARR', 'I-ARR', 'I-ARR', 'I-ARR', 'I-ARR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "IDs des labels : [4 4 4 4 1 3 3 3 3 3 4 0 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "// Encoding completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> modèle chargé avec 5 labels\n",
      "Epoch 1/3\n",
      "12/12 [==============================] - 128s 8s/step - loss: 0.5995 - accuracy: 0.7591 - val_loss: 0.3437 - val_accuracy: 0.8823\n",
      "Epoch 2/3\n",
      "12/12 [==============================] - 48s 4s/step - loss: 0.2357 - accuracy: 0.9246 - val_loss: 0.0867 - val_accuracy: 0.9828\n",
      "Epoch 3/3\n",
      "12/12 [==============================] - 49s 4s/step - loss: 0.1006 - accuracy: 0.9725 - val_loss: 0.0599 - val_accuracy: 0.9841\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0997 - accuracy: 0.9583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09974091500043869, 0.9583333134651184]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertTokenizerFast, TFBertForTokenClassification\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) # Masquer les avertissements\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # 0 = toutes les logs, 1 = info logs masqués, 2 = info et warning masqués, 3 = tout masqué\n",
    "\n",
    "file_path_train = '/home/jovyan/data/reservation-first-dataset-train.csv'\n",
    "file_path_test = '/home/jovyan/data/reservation-first-dataset-test.csv'\n",
    "data_train = pd.read_csv(file_path_train).fillna('')\n",
    "data_test = pd.read_csv(file_path_test).fillna('')\n",
    "\n",
    "print(\"\\n// Preparation of labels ['O', 'B-DEP', 'I-DEP', 'B-ARR', 'I-ARR']\")\n",
    "unique_labels = ['O', 'B-DEP', 'I-DEP', 'B-ARR', 'I-ARR']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(unique_labels)\n",
    "print(\"-> mapping labels et ids :\", {label: idx for idx, label in enumerate(label_encoder.classes_)})\n",
    "\n",
    "# Encodage des phrases et des labels correspondants\n",
    "def encode_data(data, tokenizer, label_encoder, max_length=36):  # data = CSV en input / max_length = Longueur maximale des phrases\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    \n",
    "    print(\"Starting data encoding...\")\n",
    "    for i, row in data.iterrows():\n",
    "        phrase = row['Phrase']\n",
    "        dep = row['Départ']\n",
    "        arr = row['Arrivée']\n",
    "\n",
    "        if i < 4:  # print 4 lignes\n",
    "            print(f\"\\n- Ligne {i+1}\")\n",
    "            print(\"Phrase originale :\", phrase)\n",
    "\n",
    "        dep_positions = []\n",
    "        arr_positions = []\n",
    "\n",
    "        start = 0  # Rechercher toutes les occurrences de départ\n",
    "        while True:\n",
    "            idx = phrase.find(dep, start)\n",
    "            if idx == -1:\n",
    "                break\n",
    "            dep_positions.append((idx, idx + len(dep)))\n",
    "            start = idx + len(dep)\n",
    "\n",
    "        start = 0  # Rechercher toutes les occurrences d'arrivée\n",
    "        while True:\n",
    "            idx = phrase.find(arr, start)\n",
    "            if idx == -1:\n",
    "                break\n",
    "            arr_positions.append((idx, idx + len(arr)))\n",
    "            start = idx + len(arr)\n",
    "\n",
    "        tokenized_input = tokenizer.encode_plus(\n",
    "            phrase, add_special_tokens=True, return_offsets_mapping=True, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"tf\"\n",
    "        )\n",
    "\n",
    "        tokenized_text = tokenizer.convert_ids_to_tokens(tokenized_input.input_ids[0])\n",
    "        offsets = tokenized_input['offset_mapping'].numpy()[0]\n",
    "        label_list = ['O'] * len(tokenized_text)\n",
    "\n",
    "        if i < 4:\n",
    "            print(\"Tokens encodés :\", tokenized_text)\n",
    "\n",
    "        for j, (offset_start, offset_end) in enumerate(offsets): # Assignation des labels aux tokens\n",
    "            if offset_start == 0 and offset_end == 0:\n",
    "                continue  # Token de padding\n",
    "            token_label = 'O'\n",
    "\n",
    "            # Vérifier si le token chevauche une entité de départ\n",
    "            for entity_start, entity_end in dep_positions:\n",
    "                if (offset_start >= entity_start) and (offset_end <= entity_end):\n",
    "                    if offset_start == entity_start:\n",
    "                        token_label = 'B-DEP'\n",
    "                    else:\n",
    "                        token_label = 'I-DEP'\n",
    "                    break\n",
    "\n",
    "            # Vérifier si le token chevauche une entité d'arrivée\n",
    "            for entity_start, entity_end in arr_positions:\n",
    "                if (offset_start >= entity_start) and (offset_end <= entity_end):\n",
    "                    if offset_start == entity_start:\n",
    "                        token_label = 'B-ARR'\n",
    "                    else:\n",
    "                        token_label = 'I-ARR'\n",
    "                    break\n",
    "\n",
    "            label_list[j] = token_label\n",
    "\n",
    "        if i < 4:\n",
    "            print(\"Labels après encodage des entités :\", label_list)\n",
    "\n",
    "        label_ids = label_encoder.transform(label_list)\n",
    "        tokens.append(tokenized_input.input_ids.numpy()[0])\n",
    "        labels.append(label_ids)\n",
    "\n",
    "        if i < 4:\n",
    "            print(\"IDs des labels :\", label_ids)\n",
    "\n",
    "    print(\"\\n// Encoding completed\")\n",
    "    return np.array(tokens), np.array(labels)\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased') # Version fast du tokenizer BERT\n",
    "\n",
    "print(\"\\n// Starting train data encoding...\")\n",
    "train_tokens, train_labels = encode_data(data_train, tokenizer, label_encoder, max_length=36)\n",
    "print(\"\\n// Starting test data encoding...\")\n",
    "test_tokens, test_labels = encode_data(data_test, tokenizer, label_encoder, max_length=36)\n",
    "\n",
    "\n",
    "# Chargement du modèle\n",
    "model = TFBertForTokenClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(label_encoder.classes_))\n",
    "print(\"-> modèle chargé avec\", len(label_encoder.classes_), \"labels\")\n",
    "\n",
    "# Configuration de l'entraînement \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Entraînement\n",
    "model.fit(train_tokens, train_labels, epochs=3, batch_size=16, validation_split=0.1)\n",
    "\n",
    "# Évaluation\n",
    "model.evaluate(test_tokens, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Et là avec cette opti il n'y a pas d'erreurs dans la tokenization et l'accuracy est ultra bonne.\n",
    "\n",
    "# Reste A Faire\n",
    "# - format de sortie : \n",
    "#     return f\"sentenceID, Departure: {' '.join(depart)}, Destination: {' '.join(arrivee)}\"\n",
    "#     return \"sentenceID, Code=['UNKNOWN']\"  # + la logique pour NOT_FRENCH et NOT_TRIP\n",
    "# - predictions\n",
    "# - metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
